# # -*- coding: utf-8 -*-
# """wafer defect.ipynb
#
# Automatically generated by Colab.
#
# Original file is located at
#     https://colab.research.google.com/drive/1qx49IFv_UjLt5-d4QxxpXFlPUPVjpnjJ
# """
#
# # Show wafer bin map example from the internet
#
# from IPython.display import Image
# from IPython.core.display import HTML
# Image(url= "https://www.disco.co.jp/eg/introduction/img/what_pc.png", width=600)

# Read data file
import pandas as pd


# Commented out IPython magic to ensure Python compatibility.
# Import libraries
import matplotlib.pyplot as plt
df = pd.read_pickle("C:/Users/user/Desktop/fyp/LSWMD.pkl/LSWMD.pkl")
df.info()
import numpy as np
# %matplotlib inline
import random

# Show sample of dataset
df.head()

# Correct columns
df.rename(columns={'trianTestLabel':'trainTestLabel'}, inplace=True)
df.waferIndex = df.waferIndex.astype(int)
df.tail()

# Visualize wafer index distribution
uni_Index = np.unique(df.waferIndex, return_counts=True)
plt.bar(uni_Index[0],uni_Index[1], color='gold', align='center', alpha=0.5)
plt.title("Wafer Index Distribution")
plt.xlabel("Wafer Index")
plt.ylabel("Frequency")
plt.xlim(0,26)
plt.ylim(30000,34000)
plt.show()

# Drop column for wafer index
df = df.drop(['waferIndex'], axis = 1)

# Add column for wafer map dimension
def find_dim(x):
    dim0=np.size(x,axis=0)
    dim1=np.size(x,axis=1)
    return dim0,dim1
df['waferMapDim']=df.waferMap.apply(find_dim)
df.sample(5)

# Wafer dimension distribution
#max(df.waferMapDim), min(df.waferMapDim)
uni_waferDim=np.unique(df.waferMapDim, return_counts=True)
#uni_waferDim[0].shape[0]
df['waferMapDim'].value_counts(normalize = True)

# Dataframe memory usage
df.memory_usage(deep = True)  # memory usage in bytes

# Create a copy of dataset
df2 = df.copy()
df2.failureType = df2.failureType.apply(lambda x: x[0][0] if len(x) > 0 else float("NaN"))
df2.trainTestLabel = df2.trainTestLabel.apply(lambda x: x[0][0] if len(x) > 0 else float("NaN"))
df2.sample()

# Check new dataset memory usage
df2.memory_usage(deep = True)

# Cast categorical columns

df2['trainTestLabel'] = df2['trainTestLabel'].astype('category')
df2['failureType'] = df2['failureType'].astype('category')
df2.memory_usage(deep = True)

# Check falure type distribution
df2['failureType'].value_counts(normalize=True).plot.pie(
    startangle=90, cmap="tab10", figsize=(8, 8), title="Failure Type Distribution"
)

plt.show()

# Check failure type distribution of wafers with non-null labels
#df2['failureType'].value_counts().plot.bar()
df2['failureType'].value_counts()

# Check total number of images with non-null labels
df2['failureType'].value_counts().sum()

# Delete unlabeled data and "near-full" type
import numpy as np

# Convert categorical to string to avoid errors
df2['failureType'] = df2['failureType'].astype(str)
df2['trainTestLabel'] = df2['trainTestLabel'].astype(str)

# Mapping failureType and trainTestLabel to numerical values
mapping_type = {'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3,
                'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8}
mapping_traintest = {'Training': 0, 'Test': 1}

df2['failureNum'] = df2['failureType'].map(mapping_type)
df2['trainTestNum'] = df2['trainTestLabel'].map(mapping_traintest)

# Filtering out unlabeled data and 'Near-full' type
df2 = df2[df2['failureNum'].between(0, 8)]
df2 = df2[df2['failureType'] != 'Near-full']

# Convert back to categorical and remove unused categories
df2['failureType'] = df2['failureType'].astype('category')
df2['failureType'] = df2['failureType'].cat.remove_unused_categories()

# Filtering out wafer maps that are too small
df2 = df2[df2['waferMapDim'].apply(lambda x: all(np.greater(x, (5,5))))]

# Get the number of rows
num_rows = df2.shape[0]
print(num_rows)

# Generate new dataset with balanced classes
random.seed(10)  # Set seed for reproducibility
failure_types = list(df2['failureType'].cat.categories)  # Get unique failure types
num_cat = len(failure_types)

df_list = []  # Use a list to store sampled DataFrames

for i_cat in range(num_cat):
    cat = failure_types[i_cat]
    sample = df2[df2['failureType'] == cat].sample(n=500, replace=True, random_state=10)
    df_list.append(sample)  # Append to list instead of using append()

# Use pd.concat() to merge all sampled data
df = pd.concat(df_list, ignore_index=True)

# Verify class balance
print(df['failureType'].value_counts())

# Plot 5 of each failure type

sample_size = 5
fig, axs = plt.subplots(num_cat, sample_size, figsize = (15,15))
for i_cat in range(num_cat):
    cat = failure_types[i_cat]
    random.seed(10)
    sample = df2.loc[df2['failureType'] == cat].sample(sample_size)
    for i in range(len(sample)):
        index = sample.index[i]
        axs[i_cat, i].axis('off')
        axs[i_cat, i].imshow(sample['waferMap'][index])
        axs[i_cat, i].set_title(f'{cat} - {index}')

plt.tight_layout()
plt.show()

# Look at train/test split

df['trainTestLabel'].value_counts(dropna = False, normalize = True)

# loading libraries

from scipy import ndimage

import numpy as np
import matplotlib.pyplot as plt
from scipy import ndimage

#  Drop NaNs and ensure 'failureType' is categorical
df = df.dropna(subset=['failureType'])
df['failureType'] = df['failureType'].astype('category')

# Get valid categories
failure_types = df['failureType'].cat.categories
num_cat = len(failure_types)

# Define sample size and figure layout
sample_size = 3
fig, axs = plt.subplots(num_cat, sample_size * 2, figsize=(30, 30))
axs = np.atleast_2d(axs)  # Ensure axs is always 2D

np.random.seed(10)  # Set seed for reproducibility

#  Iterate through categories
for i_cat, cat in enumerate(failure_types):
    available_rows = df[df['failureType'] == cat]

    # Use all available samples if less than required
    sample = available_rows.sample(n=min(sample_size, len(available_rows)), random_state=10)

    for i, (index, row) in enumerate(sample.iterrows()):
        original = row['waferMap']
        denoised = ndimage.median_filter(original, size=(2, 2))

        #  Plot Original
        axs[i_cat, i * 2].imshow(original)
        axs[i_cat, i * 2].axis('off')
        axs[i_cat, i * 2].set_title(f'Original \n{cat} - {index}')

        #  Plot Denoised
        axs[i_cat, i * 2 + 1].imshow(denoised, cmap='plasma')
        axs[i_cat, i * 2 + 1].axis('off')
        axs[i_cat, i * 2 + 1].set_title(f'De-noised \n{cat} - {index}')

plt.tight_layout()
plt.show()

# Apply denoising to entire dataset

import warnings
warnings.filterwarnings('ignore')
df3 = df.copy()
for i in range(len(df3['waferMap'])):
    original = df3['waferMap'].iloc[i]
    df3['waferMap'].iloc[i] = ndimage.median_filter(original, size = (2,2))

# Make the grid

an = np.linspace(0, 2*np.pi, 100)
plt.plot(2.5*np.cos(an), 2.5*np.sin(an))
plt.axis('equal')
plt.axis([-4, 4, -4, 4])
for i in range(6):
    plt.plot([-2.5, 2.5], [2.5 - i, 2.5 - i])
for i in range(6):
    plt.plot([2.5 - i, 2.5 - i], [-2.5, 2.5])

# Number the regions

for i in range(5):
    plt.text(-2.3 + i, 1.8, 1, {'color': 'C0', 'fontsize': 13})
for i in range(5):
    plt.text(1.9, 1.8 - i, 2, {'color': 'C0', 'fontsize': 13})
for i in range(5):
    plt.text(-2.3 + i, -2.2, 3, {'color': 'C0', 'fontsize': 13})
for i in range(5):
    plt.text(-2.1, 1.8 - i, 4, {'color': 'C0', 'fontsize': 13})
for row in range(3):
    for i in range(3):
        plt.text(-1.3 + i, 0.8 - row, i + 5 + row * 3, {'color': 'C0', 'fontsize': 13})

plt.title(" Devide wafer map into 13 regions")
plt.xticks([])
plt.yticks([])
plt.show()

# Define functions to calculate densities of the regions

def cal_den(x):
    return 100 * (np.sum(x==2)/np.size(x))

def find_regions(x):
    rows=np.size(x,axis=0)
    cols=np.size(x,axis=1)
    #print(cols)
    ind1=np.arange(0,rows,rows//5)
    ind2=np.arange(0,cols,cols//5)
    reg1=x[ind1[0]:ind1[1],:]
    reg2=x[:,ind2[4]:]
    reg3=x[ind1[4]:,:]
    reg4=x[:,ind2[0]:ind2[1]]
    reg5=x[ind1[1]:ind1[2],ind2[1]:ind2[2]]
    reg6=x[ind1[1]:ind1[2],ind2[2]:ind2[3]]
    reg7=x[ind1[1]:ind1[2],ind2[3]:ind2[4]]
    reg8=x[ind1[2]:ind1[3],ind2[1]:ind2[2]]
    reg9=x[ind1[2]:ind1[3],ind2[2]:ind2[3]]
    reg10=x[ind1[2]:ind1[3],ind2[3]:ind2[4]]
    reg11=x[ind1[3]:ind1[4],ind2[1]:ind2[2]]
    reg12=x[ind1[3]:ind1[4],ind2[2]:ind2[3]]
    reg13=x[ind1[3]:ind1[4],ind2[3]:ind2[4]]

    fea_reg_den = []
    fea_reg_den = [cal_den(reg1),cal_den(reg2),cal_den(reg3),cal_den(reg4),cal_den(reg5),cal_den(reg6),cal_den(reg7),cal_den(reg8),cal_den(reg9),cal_den(reg10),cal_den(reg11),cal_den(reg12),cal_den(reg13)]
    return fea_reg_den

# Apply the density calculation function

df3['fea_reg']=df3.waferMap.apply(find_regions)

import numpy as np
import matplotlib.pyplot as plt

# Ensure 'failureType' is categorical and drop NaNs
df3 = df3.dropna(subset=['failureType'])
df3['failureType'] = df3['failureType'].astype('category')

# Get unique failure types
failure_types = df3['failureType'].cat.categories
num_cat = len(failure_types)

# Set max number of plots to avoid empty subplots
num_plots = min(num_cat, 8)  # Limit to 8 plots
fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(30, 10))
axs = axs.flatten()  # Convert to 1D array

sample_size = 1  # Adjust as needed
valid_plot_count = 0  # Track the number of valid plots

#  Iterate over failure types (limit to available subplots)
for i, cat in enumerate(failure_types):
    if valid_plot_count >= num_plots:
        break  # Stop if we've filled the available subplots

    category_df = df3[df3['failureType'] == cat]

    #  Skip if no samples exist for this category
    if category_df.empty:
        print(f"Warning: No samples available for category '{cat}'. Skipping.")
        continue

    # Ensure sample size does not exceed available rows
    sample_size_actual = min(sample_size, len(category_df))
    sample = category_df.sample(sample_size_actual, random_state=10)

    # Check if 'fea_reg' column exists and has valid data
    if 'fea_reg' not in sample.columns or sample['fea_reg'].isnull().values.any():
        print(f"Warning: 'fea_reg' column missing or contains NaN for category '{cat}'. Skipping.")
        continue

    # Plot bar chart
    feature_values = list(sample['fea_reg'])[0]
    axs[valid_plot_count].bar(np.linspace(1, 13, 13), feature_values)
    axs[valid_plot_count].set_title(cat, fontsize=15)

    # Set y-axis limit dynamically
    y_lim = max(feature_values)
    axs[valid_plot_count].fill_betweenx((0, y_lim), 0, 4.5, alpha=0.5, color='g')
    axs[valid_plot_count].fill_betweenx((0, y_lim), 4.5, 8.5, alpha=0.5, color='y')
    axs[valid_plot_count].fill_betweenx((0, y_lim), 8.5, 9.5, alpha=0.5, color='r')
    axs[valid_plot_count].fill_betweenx((0, y_lim), 9.5, 13.5, alpha=0.5, color='y')

    # Label sections
    axs[valid_plot_count].text(2, y_lim, 'Edges', {'color': 'k', 'fontsize': 12})
    axs[valid_plot_count].text(4.5, y_lim * 0.9, 'Around the center', {'color': 'k', 'fontsize': 12})
    axs[valid_plot_count].text(8.5, y_lim, 'Center', {'color': 'k', 'fontsize': 12})
    axs[valid_plot_count].text(9.5, y_lim * 0.9, 'Around the center', {'color': 'k', 'fontsize': 12})

    valid_plot_count += 1  # Increment valid plot count

# Hide unused subplots
for i in range(valid_plot_count, len(axs)):
    fig.delaxes(axs[i])

plt.show()

# Change the 1s to 0s so that the wafer maps only contain two types of signals: faulty and not faulty

def change_val(img):
    img[img==1] = 0
    return img

df3['new_waferMap'] = df3.waferMap.apply(change_val)

import numpy as np
import matplotlib.pyplot as plt
from skimage.transform import radon

# Ensure 'failureType' is categorical and drop NaNs
df3 = df3.dropna(subset=['failureType', 'waferMap'])
df3['failureType'] = df3['failureType'].astype('category')

# Get unique failure types
failure_types = df3['failureType'].cat.categories
num_cat = len(failure_types)

# Dynamically set subplot grid size based on available categories
rows = (num_cat // 4) + (num_cat % 4 > 0)  # Max 4 columns per row
cols = min(num_cat, 4)  # Max 4 columns
fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(20, rows * 5))
ax = ax.flatten()  # Convert to 1D array

sample_size = 1  # Adjust as needed
valid_plot_count = 0  # Track the number of valid plots

# Iterate over failure types (limit to available subplots)
for i, cat in enumerate(failure_types):
    category_df = df3[df3['failureType'] == cat]  # Corrected DataFrame reference

    # Skip if no samples exist for this category
    if category_df.empty:
        print(f"Warning: No samples available for category '{cat}'. Skipping.")
        continue

    # Ensure sample size does not exceed available rows
    sample_size_actual = min(sample_size, len(category_df))
    sample = category_df.sample(sample_size_actual, random_state=10)

import numpy as np
import pandas as pd
from skimage import measure
from scipy import stats

# Get image
img = list(sample['waferMap'])[0]

# Ensure image is valid before applying transformation
if img is None or not isinstance(img, np.ndarray):
    print(f"Warning: Invalid image for category '{cat}'. Skipping.")


# Compute Radon transform
theta = np.linspace(0., 180., max(img.shape), endpoint=False)
sinogram = radon(img, theta=theta)

# Display transformed image
ax[valid_plot_count].imshow(sinogram, cmap=plt.cm.Greys_r, extent=(0, 180, 0, sinogram.shape[0]), aspect='auto')
ax[valid_plot_count].set_title(cat, fontsize=15)
ax[valid_plot_count].set_xticks([])

valid_plot_count += 1  # Increment valid plot count

# Hide unused subplots
for i in range(valid_plot_count, len(ax)):
    fig.delaxes(ax[i])

plt.tight_layout()
plt.show()

# Define functions for cubis interpolation

from scipy import interpolate

def cubic_inter_mean(img):
    theta = np.linspace(0., 180., max(img.shape), endpoint=False)
    sinogram = radon(img, theta=theta)
    xMean_Row = np.mean(sinogram, axis = 1)
    x = np.linspace(1, xMean_Row.size, xMean_Row.size)
    y = xMean_Row
    f = interpolate.interp1d(x, y, kind = 'cubic')
    xnew = np.linspace(1, xMean_Row.size, 20)
    ynew = f(xnew)/100   # use interpolation function returned by `interp1d`
    return ynew

def cubic_inter_std(img):
    theta = np.linspace(0., 180., max(img.shape), endpoint=False)
    sinogram = radon(img, theta=theta)
    xStd_Row = np.std(sinogram, axis=1)
    x = np.linspace(1, xStd_Row.size, xStd_Row.size)
    y = xStd_Row
    f = interpolate.interp1d(x, y, kind = 'cubic')
    xnew = np.linspace(1, xStd_Row.size, 20)
    ynew = f(xnew)/100   # use interpolation function returned by `interp1d`
    return ynew

# Apply cubic interpolation

df3['fea_cub_mean'] = df3.waferMap.apply(cubic_inter_mean)
df3['fea_cub_std'] = df3.waferMap.apply(cubic_inter_std)

import numpy as np
import matplotlib.pyplot as plt

# Remove NaN values from relevant columns
df3 = df3.dropna(subset=['failureType', 'fea_cub_mean', 'fea_cub_std'])

# Ensure 'failureType' is categorical
df3['failureType'] = df3['failureType'].astype('category')

# Get unique failure types
failure_types = df3['failureType'].cat.categories
num_cat = len(failure_types)

# Check if there are any categories to plot
if num_cat == 0:
    print("No valid failure categories found. Exiting plot generation.")
else:
    # Dynamically adjust grid size (max 4 columns)
    total_subplots = num_cat * 2  # Each category needs 2 subplots (mean + std dev)
    rows = (total_subplots // 4) + (total_subplots % 4 > 0)
    cols = min(total_subplots, 4)

    # Increase figure size for better visibility
    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(25, 25))
    ax = ax.flatten()  # Flatten the axis array for easy indexing

    plot_count = 0  # Tracks valid plots

    # Loop through failure types
    for cat in failure_types:
        # Filter for the category and check if it has valid data
        category_df = df3[df3['failureType'] == cat]
        if category_df.empty:
            print(f" Warning: No samples available for category '{cat}'. Skipping.")
            continue

        # Ensure enough samples exist
        sample_size_actual = min(len(category_df), 1)  # Prevents sampling error
        sample = category_df.sample(sample_size_actual, random_state=10)

        # Get features and check if they're valid
        fea_mean = sample['fea_cub_mean'].values[0]
        fea_std = sample['fea_cub_std'].values[0]

        if len(fea_mean) == 0 or len(fea_std) == 0:
            print(f"Warning: Invalid feature data for category '{cat}'. Skipping.")
            continue

        # Dynamically adjust y-axis limit based on data
        y_max = max(max(fea_mean), max(fea_std)) * 1.2  # Increase by 20% for visibility

        # Ensure plot count is within range before plotting
        if plot_count >= len(ax):
            print(f"Skipping additional plots due to subplot limit.")
            break

        # Plot Row Mean with larger bars
        ax[plot_count].bar(np.linspace(1, 20, 20), fea_mean, color='blue', width=0.5)
        ax[plot_count].set_title(f'{cat} - Mean', fontsize=15)
        ax[plot_count].set_xticks(range(1, 21, 2))
        ax[plot_count].set_xlim([0, 21])
        ax[plot_count].set_ylim([0, y_max])
        ax[plot_count].grid(True, linestyle='--', alpha=0.7)  # Added grid for better visibility
        plot_count += 1  # Move to the next subplot

        # Ensure plot count is within range before plotting the second subplot
        if plot_count >= len(ax):
            print(f"Skipping additional plots due to subplot limit.")
            break

        # Plot Row Standard Deviation with larger bars
        ax[plot_count].bar(np.linspace(1, 20, 20), fea_std, color='red', width=0.5)
        ax[plot_count].set_title(f'{cat} - Std Dev', fontsize=15)
        ax[plot_count].set_xticks(range(1, 21, 2))
        ax[plot_count].set_xlim([0, 21])
        ax[plot_count].set_ylim([0, y_max])
        ax[plot_count].grid(True, linestyle='--', alpha=0.7)  # ✅ Added grid for better visibility
        plot_count += 1  # Move to the next subplot

    # Remove any unused subplots (if fewer categories than available subplots)
    for i in range(plot_count, len(ax)):
        fig.delaxes(ax[i])

    plt.tight_layout()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
from skimage import measure
from scipy import stats

# Ensure 'failureType' is categorical
df3['failureType'] = df3['failureType'].astype('category')

# Get valid failure categories (exclude empty ones)
failure_types = [cat for cat in df3['failureType'].cat.categories if not df3[df3['failureType'] == cat].empty]
num_cat = len(failure_types)

# Exit early if no valid categories exist
if num_cat == 0:
    print("No valid failure categories found. Exiting plot generation.")
else:
    # Dynamically adjust rows & cols based on num_cat
    cols = min(num_cat, 4)  # Max 4 columns
    rows = -(-num_cat // cols)  # Ceiling division to get required rows

    # Create figure with exact required subplots
    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(5 * cols, 5 * rows))
    ax = np.array(ax).flatten()  # Flatten for easier indexing

    plot_count = 0  # Track the number of valid plots

    for cat in failure_types:
        # Filter data and ensure we have samples
        category_df = df3[df3['failureType'] == cat]
        if category_df.empty:
            continue  # Skip empty categories

        # Sample 1 valid entry
        sample_size_actual = min(len(category_df), 1)  # Prevents sampling errors
        sample = category_df.sample(sample_size_actual, random_state=10)

        #  Extract wafer map
        img = sample['waferMap'].values[0]
        zero_img = np.zeros(img.shape)

        #  Label connected components
        img_labels = measure.label(img, connectivity=1) - 1  # Shift to 0-based index

        # Identify most frequent region (excluding background)
        unique_labels = img_labels[img_labels > -1]
        no_region = 0 if unique_labels.size == 0 else stats.mode(unique_labels, axis=None, keepdims=True).mode.item()

        # Highlight salient region
        zero_img[np.where(img_labels == no_region)] = 2

        # Plot
        ax[plot_count].imshow(zero_img, cmap='jet')  # Better visibility
        ax[plot_count].set_title(cat, fontsize=12)
        ax[plot_count].set_xticks([])
        ax[plot_count].set_yticks([])
        plot_count += 1  # Increase count of plotted graphs

    #  Remove unused subplots
    for j in range(plot_count, len(ax)):
        fig.delaxes(ax[j])

    plt.tight_layout()
    plt.show()

# Fix measure.label() issue (remove 'neighbors' argument)
def fea_geom(img):
    # Ensure img is a valid array
    if img is None or not isinstance(img, np.ndarray):
        return None

    img_labels = measure.label(img, connectivity=1, background=0)  # Fixed

    if img_labels.max() == 0:
        img_labels[img_labels == 0] = 1  # Assign dummy label

    props = measure.regionprops(img_labels)

    if not props:
        return None  # No regions detected

    # Extract properties
    prop_area = props[0].area
    prop_perimeter = props[0].perimeter
    prop_majaxis = props[0].major_axis_length
    prop_minaxis = props[0].minor_axis_length
    prop_ecc = props[0].eccentricity
    prop_solidity = props[0].solidity

    return (prop_area, prop_perimeter, prop_majaxis, prop_minaxis, prop_ecc, prop_solidity)

# Ensure 'waferMap' has no NaN before applying fea_geom
df3 = df3.dropna(subset=['waferMap'])

# Apply function safely
df3['fea_geom'] = df3['waferMap'].apply(fea_geom)

# Combine the features

df_all=df3.copy()
a=[df_all.fea_reg.iloc[i] for i in range(df_all.shape[0])] #13
b=[df_all.fea_cub_mean.iloc[i] for i in range(df_all.shape[0])] #20
c=[df_all.fea_cub_std.iloc[i] for i in range(df_all.shape[0])] #20
d=[df_all.fea_geom.iloc[i] for i in range(df_all.shape[0])] #6
fea_all = np.concatenate((np.array(a),np.array(b),np.array(c),np.array(d)),axis=1) #59 in total

# Create target column array

label=[df_all.failureNum.iloc[i] for i in range(df_all.shape[0])]
label=np.array(label)

# Import necessary libraries
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical  # Replacing np_utils
from collections import Counter
import numpy as np

# Assuming fea_all contains feature data and label contains class labels
X = fea_all
y = label

# Split dataset into training and testing sets with stratification
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

# Print class distribution to ensure stratification worked
print('Training target statistics:', Counter(y_train))
print('Testing target statistics:', Counter(y_test))

# Convert labels to one-hot encoding for neural networks (if required)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Set a random seed for reproducibility
RANDOM_STATE = 42

# Import necessary libraries
from sklearn.svm import LinearSVC
from sklearn.multiclass import OneVsOneClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Assuming fea_all contains feature data and label contains class labels
X = fea_all
y = label

# Convert labels to integer values if not already done
if len(y.shape) > 1:
    y = np.argmax(y, axis=1)  # Convert one-hot to class labels

# Split dataset into training and testing sets with stratification
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

# Print class distribution
from collections import Counter
print('Training target statistics:', Counter(y_train))
print('Testing target statistics:', Counter(y_test))

# Initialize and train OneVsOne SVM
RANDOM_STATE = 42
SVM = OneVsOneClassifier(LinearSVC(random_state=RANDOM_STATE))
SVM.fit(X_train, y_train)

# Predictions
y_train_pred = SVM.predict(X_train)
y_test_pred = SVM.predict(X_test)

# Cross-validation (on training set, not test set)
scores = cross_val_score(SVM, X_train, y_train, cv=10, scoring='accuracy')

# Print results
print(f"Cross-validation Accuracy for Support Vector Machine: {scores.mean():.4f}")

# Define function to present results

import itertools
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    #print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    labels = failure_types
    plt.xticks(ticks = range(8), labels = labels)
    plt.yticks(ticks = range(8), labels = labels)

# Compute confusion matrix

cnf_matrix = confusion_matrix(y_test, y_test_pred)
np.set_printoptions(precision=2)

from matplotlib import gridspec
fig = plt.figure(figsize=(15, 8))
gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])

## Plot non-normalized confusion matrix

plt.subplot(gs[0])
plot_confusion_matrix(cnf_matrix, title='Confusion matrix')

# Plot normalized confusion matrix

plt.subplot(gs[1])
plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')

plt.show()

# Implement logistic regression

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression().fit(X_train, y_train)
y_train_pred = LR.predict(X_train)
y_test_pred = LR.predict(X_test)
scores = cross_val_score(LR, X_test, y_test, cv=10, scoring='accuracy')
print(f"Accuracy score for the logistic regression: \n {scores.mean()}")

# Compute confusion matrix

cnf_matrix = confusion_matrix(y_test, y_test_pred)
np.set_printoptions(precision=2)

from matplotlib import gridspec
fig = plt.figure(figsize=(15, 8))
gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])

## Plot non-normalized confusion matrix

plt.subplot(gs[0])
plot_confusion_matrix(cnf_matrix, title='Confusion matrix')

# Plot normalized confusion matrix

plt.subplot(gs[1])
plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')

plt.show()

# Implement random forest classifier

from sklearn.ensemble import RandomForestClassifier

RF = RandomForestClassifier().fit(X_train, y_train)
y_train_pred = RF.predict(X_train)
y_test_pred = RF.predict(X_test)
scores = cross_val_score(RF, X_test, y_test, cv=10, scoring='accuracy')
print(f"Accuracy score for the random forest classifier: \n {scores.mean()}")

# Compute confusion matrix

cnf_matrix = confusion_matrix(y_test, y_test_pred)
np.set_printoptions(precision=2)

from matplotlib import gridspec
fig = plt.figure(figsize=(15, 8))
gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])

## Plot non-normalized confusion matrix

plt.subplot(gs[0])
plot_confusion_matrix(cnf_matrix, title='Confusion matrix')

# Plot normalized confusion matrix

plt.subplot(gs[1])
plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')

plt.show()

# Look at parameters used by our current forest

from pprint import pprint
print('Parameters currently in use:\n')
pprint(RF.get_params())

# Hyperparemeer tuning using grid search

import random
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Set random seed for reproducibility
random.seed(10)
np.random.seed(10)

# Define the hyperparameters grid
n_estimators = [100, 300, 500]
max_depth = [30, 40, 50]
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 5]

hyperF = dict(n_estimators=n_estimators,
              max_depth=max_depth,
              min_samples_split=min_samples_split,
              min_samples_leaf=min_samples_leaf)

# Define the Random Forest model
RF = RandomForestClassifier(random_state=10)

# Perform Grid Search
gridF = GridSearchCV(RF, hyperF, cv=3, verbose=1, n_jobs=-1)
bestF = gridF.fit(X_train, y_train)  # Ensure X_train and y_train are properly defined

# Display best parameters
print("Best Hyperparameters:", gridF.best_params_)

# Look at the best parameters
best = gridF.best_params_
print(best)

# Implement random forest classifier with new parameters
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Define the Random Forest model with best hyperparameters
RF = RandomForestClassifier(
    max_depth=best['max_depth'],
    min_samples_leaf=best['min_samples_leaf'],
    min_samples_split=best['min_samples_split'],
    n_estimators=best['n_estimators'],
    random_state=10
)

# Train the model
RF.fit(X_train, y_train)

# Make predictions
y_train_pred = RF.predict(X_train)
y_test_pred = RF.predict(X_test)

# Evaluate the model using cross-validation
scores = cross_val_score(RF, X_test, y_test, cv=10, scoring='accuracy')
print(f"Accuracy score for Random Forest Classifier: {scores.mean():.4f}")

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, y_test_pred)
np.set_printoptions(precision=2)

from matplotlib import gridspec
fig = plt.figure(figsize=(15, 8))
gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])

## Plot non-normalized confusion matrix
plt.subplot(gs[0])
plot_confusion_matrix(cnf_matrix, title='Confusion matrix')

# Plot normalized confusion matrix
plt.subplot(gs[1])
plot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')

plt.show()

